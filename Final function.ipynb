{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host':'localhost', 'port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('ARC_Corpus.txt', 'r', encoding='utf-8')\n",
    "corpus_1 = open('Aristo-Mini-Corpus-Dec2016.txt', 'r', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {\"mappings\":{\n",
    "        \"properties\":{\n",
    "            \"sentence\":{\n",
    "                \"type\":\"text\"\n",
    "            }\n",
    "        }\n",
    "}\n",
    "    }\n",
    "\n",
    "ret = es.indices.create(index='corpus', ignore=400, body=b) #ARC Corpus.\n",
    "ret_2 = es.indices.create(index='corpus_1', ignore=400, body=b) #Aristo Mini Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object parallel_bulk at 0x000002A909237E48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_data_arc():\n",
    "    '''\n",
    "    This function will yield a sentence from the ARC corpus and the index name.\n",
    "    '''\n",
    "    with open('ARC_Corpus.txt', 'r', encoding='utf-8') as viola:\n",
    "        for row in viola: \n",
    "            yield{\n",
    "                \"_index\":\"corpus\",\n",
    "                 \"sentence\":row\n",
    "                 }  \n",
    "\n",
    "parallel_bulk(es, gen_data_arc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object parallel_bulk at 0x000002A909237D48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_data_aristo():\n",
    "    '''\n",
    "    This function will yield a sentence from the ARC corpus and the index name.\n",
    "    '''\n",
    "    with open('Aristo-Mini-Corpus-Dec2016.txt', 'r', encoding='utf-8') as viola:\n",
    "        for row in viola: \n",
    "            yield{\n",
    "                \"_index\":\"corpus_1\",\n",
    "                 \"sentence\":row\n",
    "                 }\n",
    "\n",
    "parallel_bulk(es, gen_data_aristo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(question):\n",
    "    '''\n",
    "    This function will take a value in question column then, will split the string \n",
    "    at (A) or (1). After splitting we will take only first part which is the question.\n",
    "    '''\n",
    "    if '(A)' in question:\n",
    "        x = question.split('(A)')[0]\n",
    "    elif '(1)' in question:\n",
    "        x = question.split('(1)')[0]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answers(question):\n",
    "    '''\n",
    "    This function will take a value in question column then, will split the string \n",
    "    at (A) or (1). After splitting we will take only first part which is the question.\n",
    "    '''\n",
    "    if '(A)' in question:\n",
    "        x = '(A) ' + question.split('(A)')[1]\n",
    "    elif '(1)' in question:\n",
    "        x = '(1) ' + question.split('(1)')[1]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(a):\n",
    "    '''\n",
    "    This function will accept a data point and returns a list of options\n",
    "    converts the only_options list of options.\n",
    "\n",
    "    Output will be as follows:\n",
    "\n",
    "    options = [option_1, option_2, option_3, option_4, option_5]                   \n",
    "\n",
    "    Note: If there's no option E then it will be written as 'None of the above'.\n",
    "    '''\n",
    "    options = []\n",
    "\n",
    "    if '(A)' in a:\n",
    "        option_1 = a.split('(B)')[0].replace('(A)','').lstrip()\n",
    "\n",
    "        b = ' '.join(i for i in a.split() if i not in option_1)\n",
    "        option_2 = b.split('(C)')[0].replace('(B)','').replace('(A)','').lstrip()\n",
    "\n",
    "        c = ' '.join(i for i in b.split() if i not in option_2)\n",
    "        option_3 = c.split('(D)')[0].replace('(C)','').replace('(B)','').replace('(A)','').lstrip()\n",
    "\n",
    "        if '(D)' not in c:\n",
    "            option_4 = 'None of the above'\n",
    "            option_5 = 'None of the above'\n",
    "        else:\n",
    "            if '(E)' not in c:\n",
    "                option_4 = c.split('(D)')[1].lstrip()\n",
    "                option_5 = 'None of the above'\n",
    "            else:\n",
    "                d = ' '.join(i for i in c.split() if i not in option_3)\n",
    "                option_4 = d.split('(E)')[0].replace('(D)','').replace('(C)','').replace('(B)','').replace('(A)','').lstrip()\n",
    "                option_5 = d.split('(E)')[1].lstrip()\n",
    "    else:\n",
    "        option_1 = a.split('(2)')[0].replace('(1)','').lstrip()\n",
    "\n",
    "        b = ' '.join(i for i in a.split() if i not in option_1)\n",
    "        option_2 = b.split('(3)')[0].replace('(2)','').replace('(1)','').lstrip()\n",
    "\n",
    "        c = ' '.join(i for i in b.split() if i not in option_2)\n",
    "        option_3 = c.split('(4)')[0].replace('(3)','').replace('(2)','').replace('(1)','').lstrip()\n",
    "\n",
    "        if '(4)' in c:\n",
    "            option_4 = c.split('(4)')[1].lstrip()\n",
    "        else:\n",
    "            option_4 = 'None of the above'\n",
    "        option_5 = 'None of the above'\n",
    "\n",
    "    options = [option_1, option_2, option_3, option_4, option_5]\n",
    "\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_for_each_candidate(question, options_list):\n",
    "    '''\n",
    "    This function will return a context after joining a question\n",
    "    and its options separately. \n",
    "    For example: question + option_1\n",
    "                 question + option_2\n",
    "                 question + option_3\n",
    "                 question + option_4\n",
    "   '''\n",
    "    option_1_docs = dict()\n",
    "    option_2_docs = dict()\n",
    "    option_3_docs = dict()\n",
    "    option_4_docs = dict()\n",
    "    option_5_docs = dict()\n",
    "    choices = options_list\n",
    "    \n",
    "    \n",
    "    for j in range(len(choices)):\n",
    "        \n",
    "        #Defining query to be searched for.\n",
    "        search_param = {\"size\": 50,\n",
    "                        \"query\": {\n",
    "                            \"bool\": {\n",
    "                                \"must\": [\n",
    "                                    {\"match\": {\n",
    "                                        \"sentence\": question + ' ' + f'{choices[j]}'\n",
    "                                    }}\n",
    "                                ],\n",
    "                                \"filter\": [\n",
    "                                    {\"match\": {\"sentence\": f'{choices[j]}'}}\n",
    "                                ]\n",
    "                            }\n",
    "                        }}\n",
    "\n",
    "        #Searching in the 'corpus' and 'corpus_1' index.\n",
    "        a = es.search(index='corpus', body=search_param)\n",
    "        b = es.search(index='corpus_1', body=search_param)\n",
    "        \n",
    "        sentences = []\n",
    "        for i in a['hits']['hits']:\n",
    "            \n",
    "            sent_tf_idf_score = dict()\n",
    "            sentence = i['_source']['sentence']\n",
    "            if sentence in sentences:\n",
    "                continue\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                score = i['_score']\n",
    "                \n",
    "                if j == 0:\n",
    "                    option_1_docs[sentence] = score\n",
    "                elif j == 1:\n",
    "                    option_2_docs[sentence] = score\n",
    "                elif j == 2:\n",
    "                    option_3_docs[sentence] = score\n",
    "                elif j == 3:\n",
    "                    option_4_docs[sentence] = score\n",
    "                elif j == 4:\n",
    "                    option_5_docs[sentence] = score\n",
    "        \n",
    "        sentences = []\n",
    "        for i in b['hits']['hits']:\n",
    "            \n",
    "            sent_tf_idf_score = dict()\n",
    "            sentence = i['_source']['sentence']\n",
    "            if sentence in sentences:\n",
    "                continue\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                score = i['_score']\n",
    "                \n",
    "                if j == 0:\n",
    "                    option_1_docs[sentence] = score\n",
    "                elif j == 1:\n",
    "                    option_2_docs[sentence] = score\n",
    "                elif j == 2:\n",
    "                    option_3_docs[sentence] = score\n",
    "                elif j == 3:\n",
    "                    option_4_docs[sentence] = score\n",
    "                elif j == 4:\n",
    "                    option_5_docs[sentence] = score\n",
    "                \n",
    "    \n",
    "    return pd.Series([option_1_docs, option_2_docs, option_3_docs, option_4_docs, option_5_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Keshav Rawat\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "tokenizer_scorer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drd_model = BertForSequenceClassification.from_pretrained('DRD_cpu', return_dict=True).to(device)\n",
    "ard_model = BertForSequenceClassification.from_pretrained('ARD_cpu', return_dict=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(question, option, document):\n",
    "    '''\n",
    "    This function will take 3 things: question, option and document.\n",
    "\n",
    "    Then it will pass question + document into DRD model and get a score.\n",
    "    It will also pass question + option + document into ARD model and get another score.\n",
    "    \n",
    "    This function returns DRD and ARD scores, i.e., logits values for label 1.\n",
    "    '''\n",
    "    ard_input_string = f'[CLS] {question} [SEP] {option} [SEP] {document} [SEP]'\n",
    "\n",
    "    drd_inputs = tokenizer_scorer(question, document, truncation='only_second', max_length=512, add_special_tokens=True, padding='max_length', return_tensors='pt')\n",
    "    drd_inputs = drd_inputs.to(device)\n",
    "\n",
    "    drd_outputs = drd_model(**drd_inputs)\n",
    "    drd_softmax = Softmax(dim=1)(drd_outputs.logits)\n",
    "    drd_score = drd_softmax.detach().cpu().numpy()[0][1]\n",
    "\n",
    "    ard_input_ids = tokenizer_scorer.encode(ard_input_string, add_special_tokens=False, truncation=True, max_length=512, return_tensors='pt')\n",
    "    ard_attention_mask = torch.ones([1,ard_input_ids.size()[1]])\n",
    "\n",
    "    ard_input_ids = ard_input_ids.to(device)\n",
    "    ard_attention_mask = ard_attention_mask.to(device)\n",
    "    ard_outputs = ard_model(input_ids=ard_input_ids, attention_mask=ard_attention_mask)\n",
    "    ard_softmax = Softmax(dim=1)(ard_outputs.logits)\n",
    "    ard_score = ard_softmax.detach().cpu().numpy()[0][1]\n",
    "\n",
    "    return drd_score, ard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_retreiver_2(dataset, only_question, options_list):\n",
    "    '''\n",
    "    This function will return context for every datapoint in the dataset.\n",
    "    ''' \n",
    "    question = only_question\n",
    "    options_list = options_list\n",
    "\n",
    "    scores = dict()\n",
    "    for j in range(len(options_list)):\n",
    "        if j == 0:\n",
    "            current_option = options_list[0]\n",
    "\n",
    "            option_1_docs = dataset[0]\n",
    "            option_1_docs_keys = list(option_1_docs.keys())\n",
    "            option_1_docs_values = list(option_1_docs.values())\n",
    "            option_1_docs_values = [float(i)+1e-6/(sum(option_1_docs_values)+1e-6) for i in option_1_docs_values]\n",
    "\n",
    "            for k in range(len(option_1_docs_keys)):\n",
    "                current_doc = option_1_docs_keys[k]\n",
    "                current_doc = re.sub('\\s+',' ', current_doc)\n",
    "                drd_score, ard_score = scorer(question, current_option, current_doc)\n",
    "\n",
    "                tf_idf_score = option_1_docs_values[k]\n",
    "\n",
    "                total_score = tf_idf_score + drd_score + ard_score\n",
    "\n",
    "                if current_doc not in scores.keys():\n",
    "                    scores[current_doc] = total_score\n",
    "                else:\n",
    "                    if total_score > scores[current_doc]:\n",
    "                        scores[current_doc] = total_score\n",
    "                    else:\n",
    "                        continue\n",
    "        elif j == 1:\n",
    "            current_option = options_list[1]\n",
    "            option_2_docs = dataset[1]\n",
    "            option_2_docs_keys = list(option_2_docs.keys())\n",
    "            option_2_docs_values = list(option_2_docs.values())\n",
    "            option_2_docs_values = [float(i)+1e-6/(sum(option_2_docs_values)+1e-6) for i in option_2_docs_values]\n",
    "\n",
    "            for k in range(len(option_2_docs_keys)):\n",
    "                current_doc = option_2_docs_keys[k]\n",
    "                current_doc = re.sub('\\s+',' ', current_doc)\n",
    "                drd_score, ard_score = scorer(question, current_option, current_doc)\n",
    "\n",
    "                tf_idf_score = option_2_docs_values[k]\n",
    "\n",
    "                total_score = tf_idf_score + drd_score + ard_score\n",
    "\n",
    "                if current_doc not in scores.keys():\n",
    "                    scores[current_doc] = total_score\n",
    "                else:\n",
    "                    if total_score > scores[current_doc]:\n",
    "                        scores[current_doc] = total_score\n",
    "                    else:\n",
    "                        continue\n",
    "        elif j == 2:\n",
    "            current_option = options_list[2]\n",
    "            option_3_docs = dataset[2]\n",
    "            option_3_docs_keys = list(option_3_docs.keys())\n",
    "            option_3_docs_values = list(option_3_docs.values())\n",
    "            option_3_docs_values = [float(i)+1e-6/(sum(option_3_docs_values)+1e-6) for i in option_3_docs_values]\n",
    "\n",
    "            for k in range(len(option_3_docs_keys)):\n",
    "                current_doc = option_3_docs_keys[k]\n",
    "                current_doc = re.sub('\\s+',' ', current_doc)\n",
    "                drd_score, ard_score = scorer(question, current_option, current_doc)\n",
    "\n",
    "                tf_idf_score = option_3_docs_values[k]\n",
    "\n",
    "                total_score = tf_idf_score + drd_score + ard_score\n",
    "\n",
    "                if current_doc not in scores.keys():\n",
    "                    scores[current_doc] = total_score\n",
    "                else:\n",
    "                    if total_score > scores[current_doc]:\n",
    "                        scores[current_doc] = total_score\n",
    "                    else:\n",
    "                        continue\n",
    "        elif j == 3:\n",
    "            current_option = options_list[3]\n",
    "            option_4_docs = dataset[3]\n",
    "            option_4_docs_keys = list(option_4_docs.keys())\n",
    "            option_4_docs_values = list(option_4_docs.values())\n",
    "            option_4_docs_values = [float(i)+1e-6/(sum(option_4_docs_values)+1e-6) for i in option_4_docs_values]\n",
    "\n",
    "            for k in range(len(option_4_docs_keys)):\n",
    "                current_doc = option_4_docs_keys[k]\n",
    "                current_doc = re.sub('\\s+',' ', current_doc)\n",
    "                drd_score, ard_score = scorer(question, current_option, current_doc)\n",
    "\n",
    "                tf_idf_score = option_4_docs_values[k]\n",
    "\n",
    "                total_score = tf_idf_score + drd_score + ard_score\n",
    "\n",
    "                if current_doc not in scores.keys():\n",
    "                    scores[current_doc] = total_score\n",
    "                else:\n",
    "                    if total_score > scores[current_doc]:\n",
    "                        scores[current_doc] = total_score\n",
    "                    else:\n",
    "                        continue\n",
    "        elif j == 4:\n",
    "            current_option = options_list[4]\n",
    "            option_5_docs = dataset[4]\n",
    "            option_5_docs_keys = list(option_5_docs.keys())\n",
    "            option_5_docs_values = list(option_5_docs.values())\n",
    "            option_5_docs_values = [float(i)+1e-6/(sum(option_5_docs_values)+1e-6) for i in option_5_docs_values]\n",
    "\n",
    "            for k in range(len(option_5_docs_keys)):\n",
    "                current_doc = option_5_docs_keys[k]\n",
    "                current_doc = re.sub('\\s+',' ', current_doc)\n",
    "                drd_score, ard_score = scorer(question, current_option, current_doc)\n",
    "\n",
    "                tf_idf_score = option_5_docs_values[k]\n",
    "\n",
    "                total_score = tf_idf_score + drd_score + ard_score\n",
    "\n",
    "                if current_doc not in scores.keys():\n",
    "                    scores[current_doc] = total_score\n",
    "                else:\n",
    "                    if total_score > scores[current_doc]:\n",
    "                        scores[current_doc] = total_score\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "        context = []\n",
    "        for l, w in enumerate(sorted(scores, key=scores.get, reverse=True)):\n",
    "            if l == 20:\n",
    "                break\n",
    "            else:\n",
    "                context.append(w)\n",
    "    joined_context = ' '.join(i for i in context)\n",
    "    return joined_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at Unified_QA_with_context and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('Unified_QA_with_context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(input_ids, **generator_args):\n",
    "    global tokenizer\n",
    "    res = model.generate(input_ids, **generator_args)\n",
    "    return [tokenizer.decode(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(array_of_input_ids, options_list):\n",
    "    '''\n",
    "    This function takes list of string. Each string is a pair of question and \n",
    "    answers separated using \\n(all lower cased). \n",
    "    For example: \n",
    "    which of the following is an example of a physical change? \\n (a) lighting a match (b) breaking a glass (c) burning of gasoline (d) rusting of iron\n",
    "\n",
    "    label_map = {'A':0,\n",
    "                'B':1,\n",
    "                'C':2,\n",
    "                'D':3,\n",
    "                'E':4,\n",
    "                '1':0,\n",
    "                '2':1,\n",
    "                '3':2,\n",
    "                '4':3,\n",
    "                '5':4}\n",
    "\n",
    "    This function will return predicted option. All answers are converted to 0, 1, 2, 3 and 4. \n",
    "\n",
    "    '''\n",
    "\n",
    "    # Converting input ids to torch tensor of Long type.\n",
    "    input_ids = array_of_input_ids.type(torch.LongTensor).to(device)\n",
    "\n",
    "    pred = run_model(input_ids.reshape((1,-1)))[0] #Reshaping because model takes input like (batch_size, sequence_length)\n",
    "\n",
    "    #Pred is in the form like '<pad> answer </s>'. So we have to remove <pad> and </s>.\n",
    "    pred = pred.replace('<pad>','')\n",
    "    pred = pred.replace('</s>','')\n",
    "    pred = pred.lstrip()\n",
    "\n",
    "    label = None\n",
    "    for j in range(len(options_list)):\n",
    "        if pred in options_list[j]:\n",
    "            label = j\n",
    "\n",
    "    if label == None:\n",
    "        label = random.randint(0,4)       \n",
    "\n",
    "    return pred, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(question):\n",
    "    '''\n",
    "    The ARC dataset contains question in the following format:\n",
    "    \n",
    "    Which factor will most likely cause a person to develop a fever?  (A) a leg muscle relaxing after exercise \\\n",
    "    (B) a bacterial population in the bloodstream (C) several viral particles on the skin \\\n",
    "    (D) carbohydrates being digested in the stomach\n",
    "    \n",
    "    As we can see here in raw form question contains options within it.\n",
    "    \n",
    "    Thus we will take question in same format.\n",
    "    '''\n",
    "    only_question = extract_question(question)\n",
    "    only_answers = extract_answers(question)\n",
    "    options_list = data_generator(only_answers)\n",
    "    \n",
    "    extracted_context = get_context_for_each_candidate(only_question, options_list)\n",
    "    \n",
    "    final_context = context_retreiver_2(extracted_context, only_question, options_list)\n",
    "    \n",
    "    input_string = only_question + '\\\\n ' + only_answers + '\\\\n ' + final_context + ' </s>'\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_string, truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "    \n",
    "    pred = predict(input_ids, options_list)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = final_fun_1('Which factor will most likely cause a person to develop a fever?  (A) a leg muscle relaxing after exercise (B) a bacterial population in the bloodstream (C) several viral particles on the skin (D) carbohydrates being digested in the stomach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bacterial population the bloodstream', 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_2(questions, labels):\n",
    "    '''\n",
    "    The ARC dataset contains question in the following format:\n",
    "    \n",
    "    Which factor will most likely cause a person to develop a fever?  (A) a leg muscle relaxing after exercise \\\n",
    "    (B) a bacterial population in the bloodstream (C) several viral particles on the skin \\\n",
    "    (D) carbohydrates being digested in the stomach\n",
    "    \n",
    "    As we can see here in raw form question contains options within it.\n",
    "    \n",
    "    Thus we will take question in same format.\n",
    "    '''\n",
    "    pred_labels = []\n",
    "    \n",
    "    for i in range(len(questions)):\n",
    "        \n",
    "        only_question = extract_question(questions[i])\n",
    "        only_answers = extract_answers(questions[i])\n",
    "        options_list = data_generator(only_answers)\n",
    "\n",
    "        extracted_context = get_context_for_each_candidate(only_question, options_list)\n",
    "\n",
    "        final_context = context_retreiver_2(extracted_context, only_question, options_list)\n",
    "\n",
    "        input_string = only_question + '\\\\n ' + only_answers + '\\\\n ' + final_context + ' </s>'\n",
    "\n",
    "        input_ids = tokenizer.encode(input_string, truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "\n",
    "        pred, label = predict(input_ids, options_list)\n",
    "        \n",
    "        pred_labels.append(label)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, pred_labels)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['Which factor will most likely cause a person to develop a fever?  (A) a leg muscle relaxing after exercise (B) a bacterial population in the bloodstream (C) several viral particles on the skin (D) carbohydrates being digested in the stomach',\n",
    "            'Lichens are symbiotic organisms made of green algae and fungi. What do the green algae supply to the fungi in this symbiotic relationship? (A) carbon dioxide (B) food (C) protection (D) water',\n",
    "            'Rocks are classified as igneous, metamorphic, or sedimentary according to (1) their color (2) their shape (3) how they formed (4) the minerals they contain']\n",
    "labels = [1,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = final_fun_2(questions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "By undertaking this project I found following things:\n",
    "\n",
    "1. Model size does have affect on the final performance.\n",
    "2. Context is very important to answer the questions. Thus better context retreival techniques should be used.\n",
    "3. By adding dicriminators we got better performance on the cost of computational speed.\n",
    "4. Transfer learning in NLP has made many things easy to accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ways by which perfomance can be improved. Some of them are as follows:\n",
    "\n",
    "1. Using bigger models. I have used only base models due to less computational resources.\n",
    "2. For scoring context using ARD and DRD, we can use more than one DRD or ARD and take their average scores as final scores.\n",
    "3. Instead of taking average of ARD and DRD scores, we can use KV attention to get better context(as described in Attentive Ranker paper).\n",
    "4. Try with decoder based models like GPT, GPT-2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
